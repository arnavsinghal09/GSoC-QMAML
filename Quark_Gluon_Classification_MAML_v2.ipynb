{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNu+p7egZEgYgNa51heGao/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavsinghal09/GSoC-QMAML/blob/main/Quark_Gluon_Classification_QMAML_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSSNKRvxVGfC",
        "outputId": "50fe8b97-1ae6-4072-af16-1862b087f21c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import random"
      ],
      "metadata": {
        "id": "aXBBLNaDY9rw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Hyperparameters\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/quark-gluon-dataset\"\n",
        "FILES = [\n",
        "    \"quark-gluon_train-set_n793900.hdf5\",\n",
        "    \"quark-gluon_test-set_n139306.hdf5\",\n",
        "    \"quark-gluon_test-set_n10000.hdf5\"\n",
        "]\n",
        "FILE_PATHS = [f\"{DATA_DIR}/{fname}\" for fname in FILES]\n",
        "FILE_LABELS = [\"Train\", \"Test1\", \"Test2\"]\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 5e-4\n",
        "EPOCHS = 20\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMG_SHAPE = (3, 125, 125)\n",
        "N_WAY = 2\n",
        "K_SHOT = 32\n",
        "K_QUERY = 64\n",
        "META_BATCH_SIZE = 8\n",
        "INNER_STEPS = 3\n",
        "INNER_LR = 5e-3\n",
        "NUM_WORKERS = 2"
      ],
      "metadata": {
        "id": "2syvlwAYZRoz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Dataset and Task Sampler\n",
        "\n",
        "class JetImageDataset(Dataset):\n",
        "    def __init__(self, X, y, pt=None, pt_bins=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.pt = pt\n",
        "        self.pt_bins = pt_bins\n",
        "        if self.pt is not None and self.pt_bins is not None:\n",
        "            self.bin_indices = []\n",
        "            for i in range(len(pt_bins) - 1):\n",
        "                idx = np.where((pt >= pt_bins[i]) & (pt < pt_bins[i+1]))[0]\n",
        "                self.bin_indices.append(idx)\n",
        "        else:\n",
        "            self.bin_indices = [np.arange(len(y))]\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        if x.shape != IMG_SHAPE:\n",
        "            x = np.transpose(x, (2, 0, 1))  # (3, 125, 125)\n",
        "        label = int(self.y[idx])\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "def sample_task(dataset, bin_idx, k_shot, k_query):\n",
        "    idxs = dataset.bin_indices[bin_idx]\n",
        "    idxs = np.random.permutation(idxs)\n",
        "    support_idxs = idxs[:k_shot]\n",
        "    query_idxs = idxs[k_shot:k_shot + k_query]\n",
        "    X_s, y_s = zip(*[dataset[i] for i in support_idxs])\n",
        "    X_q, y_q = zip(*[dataset[i] for i in query_idxs])\n",
        "    return (torch.stack(X_s), torch.tensor(y_s)), (torch.stack(X_q), torch.tensor(y_q))"
      ],
      "metadata": {
        "id": "d-J4ymZFmeKd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Model (Deeper CNN for Jet Images)\n",
        "\n",
        "class JetCNN(nn.Module):\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 5, stride=2, padding=2), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, 512), nn.ReLU(),\n",
        "            nn.Linear(512, n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "WueLeVuHvvAX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Data Preparation and Meta-Task Definition\n",
        "\n",
        "with h5py.File(FILE_PATHS[0], \"r\") as f:\n",
        "    X = np.array(f[\"X_jets\"][:50000])  # Use more data for better GPU usage\n",
        "    y = np.array(f[\"y\"][:50000])\n",
        "    pt = np.array(f[\"pt\"][:50000])\n",
        "\n",
        "pt_bins = np.percentile(pt, np.linspace(0, 100, 6))\n",
        "print(\"pT bins:\", pt_bins)\n",
        "\n",
        "jet_dataset = JetImageDataset(X, y, pt=pt, pt_bins=pt_bins)\n",
        "\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "split = int(0.8 * len(indices))\n",
        "train_idx, val_idx = indices[:split], indices[split:]\n",
        "train_loader = DataLoader(\n",
        "    Subset(jet_dataset, train_idx),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    Subset(jet_dataset, val_idx),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_sSr12cmiJK",
        "outputId": "1985b192-4bb0-4a68-c955-03d474846b40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pT bins: [ 70.23306274  95.28910828 105.83418427 117.69684753 135.86664734\n",
            " 308.84353638]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Training Loop for Classical Baseline (with All Stats)\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    return acc, f1, prec, rec\n",
        "\n",
        "def train_baseline(model, train_loader, val_loader, epochs=EPOCHS):\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        y_true_train, y_pred_train = [], []\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss = loss_fn(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * X.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            y_true_train.append(y)\n",
        "            y_pred_train.append(preds)\n",
        "        y_true_train = torch.cat(y_true_train)\n",
        "        y_pred_train = torch.cat(y_pred_train)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc, train_f1, train_prec, train_rec = compute_metrics(y_true_train, y_pred_train)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        y_true_val, y_pred_val = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss = loss_fn(logits, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                preds = logits.argmax(dim=1)\n",
        "                y_true_val.append(y)\n",
        "                y_pred_val.append(preds)\n",
        "        y_true_val = torch.cat(y_true_val)\n",
        "        y_pred_val = torch.cat(y_pred_val)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc, val_f1, val_prec, val_rec = compute_metrics(y_true_val, y_pred_val)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f} | \"\n",
        "              f\"Prec: {train_prec:.4f} | Rec: {train_rec:.4f} || \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f} | \"\n",
        "              f\"Prec: {val_prec:.4f} | Rec: {val_rec:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "mOkm3tXwndH0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. MAML Meta-Learning Loop (with All Stats)\n",
        "\n",
        "def maml_train(model, dataset, pt_bins, meta_batch_size=META_BATCH_SIZE, epochs=EPOCHS):\n",
        "    model = model.to(DEVICE)\n",
        "    meta_optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        meta_loss = 0\n",
        "        all_y_true, all_y_pred = [], []\n",
        "        for _ in range(meta_batch_size):\n",
        "            bin_idx = random.randint(0, len(pt_bins)-2)\n",
        "            (X_s, y_s), (X_q, y_q) = sample_task(dataset, bin_idx, K_SHOT, K_QUERY)\n",
        "            X_s, y_s, X_q, y_q = X_s.to(DEVICE), y_s.to(DEVICE), X_q.to(DEVICE), y_q.to(DEVICE)\n",
        "            # Clone model for inner loop\n",
        "            fast_weights = [p.clone().detach().requires_grad_(True) for p in model.parameters()]\n",
        "            # Inner loop\n",
        "            for _ in range(INNER_STEPS):\n",
        "                logits = model(X_s)\n",
        "                loss = loss_fn(logits, y_s)\n",
        "                grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "                fast_weights = [w - INNER_LR * g for w, g in zip(fast_weights, grads)]\n",
        "            # Outer loop: evaluate on query set\n",
        "            def forward_with_weights(x, weights):\n",
        "                x = nn.functional.conv2d(x, weights[0], weights[1], stride=2, padding=2)\n",
        "                x = nn.functional.relu(x)\n",
        "                x = nn.functional.conv2d(x, weights[2], weights[3], stride=2, padding=1)\n",
        "                x = nn.functional.relu(x)\n",
        "                x = nn.functional.conv2d(x, weights[4], weights[5], stride=2, padding=1)\n",
        "                x = nn.functional.relu(x)\n",
        "                x = nn.functional.conv2d(x, weights[6], weights[7], stride=2, padding=1)\n",
        "                x = nn.functional.relu(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = nn.functional.linear(x, weights[8], weights[9])\n",
        "                x = nn.functional.relu(x)\n",
        "                x = nn.functional.linear(x, weights[10], weights[11])\n",
        "                return x\n",
        "            logits_q = forward_with_weights(X_q, fast_weights)\n",
        "            loss_q = loss_fn(logits_q, y_q)\n",
        "            meta_loss += loss_q\n",
        "            preds = logits_q.argmax(dim=1)\n",
        "            all_y_true.append(y_q)\n",
        "            all_y_pred.append(preds)\n",
        "        meta_optimizer.zero_grad()\n",
        "        meta_loss.backward()\n",
        "        meta_optimizer.step()\n",
        "        all_y_true = torch.cat(all_y_true)\n",
        "        all_y_pred = torch.cat(all_y_pred)\n",
        "        acc, f1, prec, rec = compute_metrics(all_y_true, all_y_pred)\n",
        "        print(f\"Epoch {epoch+1:2d} | Meta Loss: {meta_loss.item()/meta_batch_size:.4f} | \"\n",
        "              f\"Acc: {acc:.4f} | F1: {f1:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "A5-an-WPne8h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train and Evaluate Baseline\n",
        "\n",
        "baseline_model = JetCNN()\n",
        "baseline_model = train_baseline(baseline_model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igD1nB40ng6C",
        "outputId": "722fd6f8-ae66-4406-ef6f-d70eea9e2a79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | Train Loss: 0.6935 | Acc: 0.4951 | F1: 0.2211 | Prec: 0.4867 | Rec: 0.1430 || Val Loss: 0.6931 | Acc: 0.5041 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2 | Train Loss: 0.6345 | Acc: 0.6364 | F1: 0.6496 | Prec: 0.6280 | Rec: 0.6729 || Val Loss: 0.5966 | Acc: 0.6897 | F1: 0.6776 | Prec: 0.6989 | Rec: 0.6576\n",
            "Epoch  3 | Train Loss: 0.5943 | Acc: 0.6928 | F1: 0.7059 | Prec: 0.6784 | Rec: 0.7358 || Val Loss: 0.5942 | Acc: 0.6950 | F1: 0.6962 | Prec: 0.6879 | Rec: 0.7046\n",
            "Epoch  4 | Train Loss: 0.5887 | Acc: 0.6973 | F1: 0.7073 | Prec: 0.6860 | Rec: 0.7300 || Val Loss: 0.5912 | Acc: 0.6972 | F1: 0.7110 | Prec: 0.6749 | Rec: 0.7512\n",
            "Epoch  5 | Train Loss: 0.5862 | Acc: 0.6977 | F1: 0.7079 | Prec: 0.6861 | Rec: 0.7311 || Val Loss: 0.5955 | Acc: 0.6940 | F1: 0.7145 | Prec: 0.6649 | Rec: 0.7721\n",
            "Epoch  6 | Train Loss: 0.5837 | Acc: 0.7007 | F1: 0.7108 | Prec: 0.6888 | Rec: 0.7342 || Val Loss: 0.5898 | Acc: 0.6984 | F1: 0.7097 | Prec: 0.6789 | Rec: 0.7433\n",
            "Epoch  7 | Train Loss: 0.5814 | Acc: 0.7023 | F1: 0.7125 | Prec: 0.6903 | Rec: 0.7361 || Val Loss: 0.5867 | Acc: 0.6973 | F1: 0.7010 | Prec: 0.6870 | Rec: 0.7157\n",
            "Epoch  8 | Train Loss: 0.5779 | Acc: 0.7042 | F1: 0.7117 | Prec: 0.6953 | Rec: 0.7289 || Val Loss: 0.5877 | Acc: 0.7008 | F1: 0.6894 | Prec: 0.7104 | Rec: 0.6697\n",
            "Epoch  9 | Train Loss: 0.5736 | Acc: 0.7074 | F1: 0.7142 | Prec: 0.6994 | Rec: 0.7295 || Val Loss: 0.5831 | Acc: 0.7003 | F1: 0.7016 | Prec: 0.6929 | Rec: 0.7106\n",
            "Epoch 10 | Train Loss: 0.5698 | Acc: 0.7116 | F1: 0.7174 | Prec: 0.7045 | Rec: 0.7307 || Val Loss: 0.5758 | Acc: 0.7101 | F1: 0.7150 | Prec: 0.6976 | Rec: 0.7332\n",
            "Epoch 11 | Train Loss: 0.5645 | Acc: 0.7170 | F1: 0.7215 | Prec: 0.7118 | Rec: 0.7314 || Val Loss: 0.5779 | Acc: 0.7061 | F1: 0.7286 | Prec: 0.6720 | Rec: 0.7957\n",
            "Epoch 12 | Train Loss: 0.5574 | Acc: 0.7218 | F1: 0.7257 | Prec: 0.7170 | Rec: 0.7346 || Val Loss: 0.5669 | Acc: 0.7158 | F1: 0.7203 | Prec: 0.7034 | Rec: 0.7381\n",
            "Epoch 13 | Train Loss: 0.5518 | Acc: 0.7282 | F1: 0.7311 | Prec: 0.7249 | Rec: 0.7374 || Val Loss: 0.5654 | Acc: 0.7216 | F1: 0.7199 | Prec: 0.7183 | Rec: 0.7215\n",
            "Epoch 14 | Train Loss: 0.5466 | Acc: 0.7315 | F1: 0.7331 | Prec: 0.7302 | Rec: 0.7361 || Val Loss: 0.5741 | Acc: 0.7126 | F1: 0.7261 | Prec: 0.6884 | Rec: 0.7681\n",
            "Epoch 15 | Train Loss: 0.5445 | Acc: 0.7323 | F1: 0.7338 | Prec: 0.7313 | Rec: 0.7362 || Val Loss: 0.5786 | Acc: 0.7131 | F1: 0.7351 | Prec: 0.6780 | Rec: 0.8026\n",
            "Epoch 16 | Train Loss: 0.5377 | Acc: 0.7379 | F1: 0.7394 | Prec: 0.7367 | Rec: 0.7422 || Val Loss: 0.5658 | Acc: 0.7232 | F1: 0.7229 | Prec: 0.7177 | Rec: 0.7282\n",
            "Epoch 17 | Train Loss: 0.5328 | Acc: 0.7421 | F1: 0.7433 | Prec: 0.7413 | Rec: 0.7454 || Val Loss: 0.5802 | Acc: 0.7171 | F1: 0.7369 | Prec: 0.6839 | Rec: 0.7987\n",
            "Epoch 18 | Train Loss: 0.5330 | Acc: 0.7412 | F1: 0.7427 | Prec: 0.7400 | Rec: 0.7454 || Val Loss: 0.5707 | Acc: 0.7229 | F1: 0.7125 | Prec: 0.7338 | Rec: 0.6925\n",
            "Epoch 19 | Train Loss: 0.5253 | Acc: 0.7463 | F1: 0.7462 | Prec: 0.7479 | Rec: 0.7446 || Val Loss: 0.5733 | Acc: 0.7234 | F1: 0.7244 | Prec: 0.7160 | Rec: 0.7330\n",
            "Epoch 20 | Train Loss: 0.5202 | Acc: 0.7493 | F1: 0.7492 | Prec: 0.7512 | Rec: 0.7472 || Val Loss: 0.5782 | Acc: 0.7238 | F1: 0.7161 | Prec: 0.7303 | Rec: 0.7024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train and Evaluate MAML\n",
        "\n",
        "maml_model = JetCNN()\n",
        "maml_model = maml_train(maml_model, jet_dataset, pt_bins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPSsrcFjnisV",
        "outputId": "da26ca85-1cca-4952-dfb7-02fad9196186"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | Meta Loss: 0.6927 | Acc: 0.5117 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n",
            "Epoch  2 | Meta Loss: 0.6983 | Acc: 0.5000 | F1: 0.6667 | Prec: 0.5000 | Rec: 1.0000\n",
            "Epoch  3 | Meta Loss: 0.7421 | Acc: 0.5215 | F1: 0.6855 | Prec: 0.5215 | Rec: 1.0000\n",
            "Epoch  4 | Meta Loss: 0.8504 | Acc: 0.4746 | F1: 0.6437 | Prec: 0.4746 | Rec: 1.0000\n",
            "Epoch  5 | Meta Loss: 0.9558 | Acc: 0.4844 | F1: 0.6526 | Prec: 0.4844 | Rec: 1.0000\n",
            "Epoch  6 | Meta Loss: 0.7888 | Acc: 0.5039 | F1: 0.6701 | Prec: 0.5039 | Rec: 1.0000\n",
            "Epoch  7 | Meta Loss: 0.7572 | Acc: 0.4531 | F1: 0.6237 | Prec: 0.4531 | Rec: 1.0000\n",
            "Epoch  8 | Meta Loss: 0.7116 | Acc: 0.5078 | F1: 0.2174 | Prec: 0.5469 | Rec: 0.1357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9 | Meta Loss: 0.7959 | Acc: 0.4785 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Meta Loss: 0.7976 | Acc: 0.5312 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Meta Loss: 0.8223 | Acc: 0.4785 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Meta Loss: 0.7412 | Acc: 0.5371 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n",
            "Epoch 13 | Meta Loss: 0.7148 | Acc: 0.5254 | F1: 0.2085 | Prec: 0.5000 | Rec: 0.1317\n",
            "Epoch 14 | Meta Loss: 0.7019 | Acc: 0.5371 | F1: 0.4552 | Prec: 0.5156 | Rec: 0.4074\n",
            "Epoch 15 | Meta Loss: 0.6993 | Acc: 0.5195 | F1: 0.4252 | Prec: 0.4740 | Rec: 0.3856\n",
            "Epoch 16 | Meta Loss: 0.6981 | Acc: 0.4844 | F1: 0.4027 | Prec: 0.4635 | Rec: 0.3560\n",
            "Epoch 17 | Meta Loss: 0.6880 | Acc: 0.5645 | F1: 0.5584 | Prec: 0.5508 | Rec: 0.5663\n",
            "Epoch 18 | Meta Loss: 0.6932 | Acc: 0.5234 | F1: 0.6176 | Prec: 0.5130 | Rec: 0.7756\n",
            "Epoch 19 | Meta Loss: 0.7414 | Acc: 0.4668 | F1: 0.4699 | Prec: 0.4727 | Rec: 0.4672\n",
            "Epoch 20 | Meta Loss: 0.7072 | Acc: 0.5078 | F1: 0.6135 | Prec: 0.5208 | Rec: 0.7463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Few-Shot Evaluation Function\n",
        "\n",
        "def few_shot_eval(model, dataset, pt_bins, n_tasks=None, k_shot=K_SHOT, k_query=K_QUERY, inner_steps=INNER_STEPS, inner_lr=INNER_LR):\n",
        "    model.eval()\n",
        "    all_acc, all_f1, all_prec, all_rec = [], [], [], []\n",
        "    n_bins = len(pt_bins) - 1 if n_tasks is None else n_tasks\n",
        "    for bin_idx in range(n_bins):\n",
        "        (X_s, y_s), (X_q, y_q) = sample_task(dataset, bin_idx, k_shot, k_query)\n",
        "        X_s, y_s, X_q, y_q = X_s.to(DEVICE), y_s.to(DEVICE), X_q.to(DEVICE), y_q.to(DEVICE)\n",
        "        # Fast adaptation (inner loop)\n",
        "        fast_weights = [p.clone().detach().requires_grad_(True) for p in model.parameters()]\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for _ in range(inner_steps):\n",
        "            logits = model(X_s)\n",
        "            loss = loss_fn(logits, y_s)\n",
        "            grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "            fast_weights = [w - inner_lr * g for w, g in zip(fast_weights, grads)]\n",
        "        # Evaluate on query set\n",
        "        def forward_with_weights(x, weights):\n",
        "            x = nn.functional.conv2d(x, weights[0], weights[1], stride=2, padding=2)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = nn.functional.conv2d(x, weights[2], weights[3], stride=2, padding=1)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = nn.functional.conv2d(x, weights[4], weights[5], stride=2, padding=1)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = nn.functional.conv2d(x, weights[6], weights[7], stride=2, padding=1)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = nn.functional.linear(x, weights[8], weights[9])\n",
        "            x = nn.functional.relu(x)\n",
        "            x = nn.functional.linear(x, weights[10], weights[11])\n",
        "            return x\n",
        "        logits_q = forward_with_weights(X_q, fast_weights)\n",
        "        preds = logits_q.argmax(dim=1)\n",
        "        acc = accuracy_score(y_q.cpu().numpy(), preds.cpu().numpy())\n",
        "        f1 = f1_score(y_q.cpu().numpy(), preds.cpu().numpy())\n",
        "        prec = precision_score(y_q.cpu().numpy(), preds.cpu().numpy())\n",
        "        rec = recall_score(y_q.cpu().numpy(), preds.cpu().numpy())\n",
        "        all_acc.append(acc)\n",
        "        all_f1.append(f1)\n",
        "        all_prec.append(prec)\n",
        "        all_rec.append(rec)\n",
        "        print(f\"Bin {bin_idx}: Acc={acc:.4f} | F1={f1:.4f} | Prec={prec:.4f} | Rec={rec:.4f}\")\n",
        "    print(f\"\\nMean Few-Shot: Acc={np.mean(all_acc):.4f} | F1={np.mean(all_f1):.4f} | Prec={np.mean(all_prec):.4f} | Rec={np.mean(all_rec):.4f}\")\n",
        "    return all_acc, all_f1, all_prec, all_rec"
      ],
      "metadata": {
        "id": "UDHNCW8Fw5hX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Few-Shot Evaluation for MAML and Baseline\n",
        "\n",
        "print(\"MAML Few-Shot Evaluation:\")\n",
        "few_shot_eval(maml_model, jet_dataset, pt_bins)\n",
        "\n",
        "print(\"\\nClassical Baseline Few-Shot Evaluation (no adaptation):\")\n",
        "# For baseline, skip adaptation: just forward pass\n",
        "def baseline_few_shot_eval(model, dataset, pt_bins, n_tasks=None, k_query=K_QUERY):\n",
        "    model.eval()\n",
        "    all_acc, all_f1, all_prec, all_rec = [], [], [], []\n",
        "    n_bins = len(pt_bins) - 1 if n_tasks is None else n_tasks\n",
        "    for bin_idx in range(n_bins):\n",
        "        idxs = dataset.bin_indices[bin_idx]\n",
        "        idxs = np.random.permutation(idxs)\n",
        "        query_idxs = idxs[:k_query]\n",
        "        X_q, y_q = zip(*[dataset[i] for i in query_idxs])\n",
        "        X_q = torch.stack(X_q).to(DEVICE)\n",
        "        y_q = torch.tensor(y_q).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            logits_q = model(X_q)\n",
        "            preds = logits_q.argmax(dim=1)\n",
        "        acc = accuracy_score(y_q.cpu().numpy(), preds.cpu().numpy())\n",
        "        f1 = f1_score(y_q.cpu().numpy(), preds.cpu().numpy())\n",
        "        prec = precision_score(y_q.cpu().numpy(), preds.cpu().numpy())\n",
        "        rec = recall_score(y_q.cpu().numpy(), preds.cpu().numpy())\n",
        "        all_acc.append(acc)\n",
        "        all_f1.append(f1)\n",
        "        all_prec.append(prec)\n",
        "        all_rec.append(rec)\n",
        "        print(f\"Bin {bin_idx}: Acc={acc:.4f} | F1={f1:.4f} | Prec={prec:.4f} | Rec={rec:.4f}\")\n",
        "    print(f\"\\nMean Few-Shot: Acc={np.mean(all_acc):.4f} | F1={np.mean(all_f1):.4f} | Prec={np.mean(all_prec):.4f} | Rec={np.mean(all_rec):.4f}\")\n",
        "    return all_acc, all_f1, all_prec, all_rec\n",
        "\n",
        "baseline_few_shot_eval(baseline_model, jet_dataset, pt_bins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnjrKesBw7H_",
        "outputId": "6b3ab025-ad26-43f9-e301-4f062606e056"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAML Few-Shot Evaluation:\n",
            "Bin 0: Acc=0.5625 | F1=0.0000 | Prec=0.0000 | Rec=0.0000\n",
            "Bin 1: Acc=0.6094 | F1=0.7573 | Prec=0.6094 | Rec=1.0000\n",
            "Bin 2: Acc=0.5000 | F1=0.6667 | Prec=0.5000 | Rec=1.0000\n",
            "Bin 3: Acc=0.3906 | F1=0.0000 | Prec=0.0000 | Rec=0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bin 4: Acc=0.4844 | F1=0.6526 | Prec=0.4844 | Rec=1.0000\n",
            "\n",
            "Mean Few-Shot: Acc=0.5094 | F1=0.4153 | Prec=0.3187 | Rec=0.6000\n",
            "\n",
            "Classical Baseline Few-Shot Evaluation (no adaptation):\n",
            "Bin 0: Acc=0.7031 | F1=0.6275 | Prec=0.6154 | Rec=0.6400\n",
            "Bin 1: Acc=0.7031 | F1=0.7164 | Prec=0.8571 | Rec=0.6154\n",
            "Bin 2: Acc=0.7656 | F1=0.7887 | Prec=0.7778 | Rec=0.8000\n",
            "Bin 3: Acc=0.7969 | F1=0.8116 | Prec=0.8750 | Rec=0.7568\n",
            "Bin 4: Acc=0.7188 | F1=0.7188 | Prec=0.6571 | Rec=0.7931\n",
            "\n",
            "Mean Few-Shot: Acc=0.7375 | F1=0.7326 | Prec=0.7565 | Rec=0.7210\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.703125, 0.703125, 0.765625, 0.796875, 0.71875],\n",
              " [0.6274509803921569,\n",
              "  0.7164179104477612,\n",
              "  0.7887323943661971,\n",
              "  0.8115942028985508,\n",
              "  0.71875],\n",
              " [0.6153846153846154,\n",
              "  0.8571428571428571,\n",
              "  0.7777777777777778,\n",
              "  0.875,\n",
              "  0.6571428571428571],\n",
              " [0.64, 0.6153846153846154, 0.8, 0.7567567567567568, 0.7931034482758621])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}
