{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjLGP7W70ZxkoCIDv2QqCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavsinghal09/GSoC-QMAML/blob/main/Quark_Gluon_Classification_MAML_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSSNKRvxVGfC",
        "outputId": "2d146a10-7746-4a82-f806-49751c43c0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports and Hyperparameters\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import random\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/quark-gluon-dataset\"\n",
        "FILES = [\n",
        "    \"quark-gluon_train-set_n793900.hdf5\",\n",
        "    \"quark-gluon_test-set_n139306.hdf5\",\n",
        "    \"quark-gluon_test-set_n10000.hdf5\"\n",
        "]\n",
        "FILE_PATHS = [f\"{DATA_DIR}/{fname}\" for fname in FILES]\n",
        "FILE_LABELS = [\"Train\", \"Test1\", \"Test2\"]\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMG_SHAPE = (3, 125, 125)  # For jet images\n",
        "N_WAY = 2\n",
        "K_SHOT = 16\n",
        "K_QUERY = 32\n",
        "META_BATCH_SIZE = 4\n",
        "INNER_STEPS = 1\n",
        "INNER_LR = 1e-2"
      ],
      "metadata": {
        "id": "aXBBLNaDY9rw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1. Jet Image Dataset\n",
        "\n",
        "class JetImageDataset(Dataset):\n",
        "    def __init__(self, X, y, pt=None, pt_bins=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.pt = pt\n",
        "        self.pt_bins = pt_bins\n",
        "        if self.pt is not None and self.pt_bins is not None:\n",
        "            self.bin_indices = []\n",
        "            for i in range(len(pt_bins) - 1):\n",
        "                idx = np.where((pt >= pt_bins[i]) & (pt < pt_bins[i+1]))[0]\n",
        "                self.bin_indices.append(idx)\n",
        "        else:\n",
        "            self.bin_indices = [np.arange(len(y))]\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "      x = self.X[idx]\n",
        "      if x.shape != IMG_SHAPE:\n",
        "          x = np.transpose(x, (2, 0, 1))  # (3, 125, 125)\n",
        "      label = int(self.y[idx])  # <-- Cast to int here!\n",
        "      return torch.tensor(x, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
      ],
      "metadata": {
        "id": "2syvlwAYZRoz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2. Task Sampler\n",
        "\n",
        "def sample_task(dataset, bin_idx, k_shot, k_query):\n",
        "    idxs = dataset.bin_indices[bin_idx]\n",
        "    idxs = np.random.permutation(idxs)\n",
        "    support_idxs = idxs[:k_shot]\n",
        "    query_idxs = idxs[k_shot:k_shot + k_query]\n",
        "    X_s, y_s = zip(*[dataset[i] for i in support_idxs])\n",
        "    X_q, y_q = zip(*[dataset[i] for i in query_idxs])\n",
        "    return (torch.stack(X_s), torch.tensor(y_s)), (torch.stack(X_q), torch.tensor(y_q))"
      ],
      "metadata": {
        "id": "d-J4ymZFmeKd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1. CNN for Jet Images\n",
        "\n",
        "class JetCNN(nn.Module):\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 5, stride=2, padding=2), nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 16 * 16, 128), nn.ReLU(),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "g_sSr12cmiJK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Classical Supervised Baseline\n",
        "\n",
        "def train_baseline(model, train_loader, val_loader, epochs=EPOCHS):\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss = loss_fn(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                preds = model(X).argmax(dim=1)\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.size(0)\n",
        "        print(f\"Epoch {epoch+1}: Val Acc = {correct/total:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "mOkm3tXwndH0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. MAML Meta-Learning Loop\n",
        "\n",
        "def maml_train(model, dataset, pt_bins, meta_batch_size=META_BATCH_SIZE, epochs=EPOCHS):\n",
        "    model = model.to(DEVICE)\n",
        "    meta_optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        meta_loss = 0\n",
        "        for _ in range(meta_batch_size):\n",
        "            # Sample a meta-task (random pt bin)\n",
        "            bin_idx = random.randint(0, len(pt_bins)-2)\n",
        "            (X_s, y_s), (X_q, y_q) = sample_task(dataset, bin_idx, K_SHOT, K_QUERY)\n",
        "            X_s, y_s, X_q, y_q = X_s.to(DEVICE), y_s.to(DEVICE), X_q.to(DEVICE), y_q.to(DEVICE)\n",
        "            # Clone model for inner loop\n",
        "            fast_weights = [p.clone().detach().requires_grad_(True) for p in model.parameters()]\n",
        "            # Inner loop\n",
        "            for _ in range(INNER_STEPS):\n",
        "                logits = model(X_s)\n",
        "                loss = loss_fn(logits, y_s)\n",
        "                grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "                fast_weights = [w - INNER_LR * g for w, g in zip(fast_weights, grads)]\n",
        "            # Outer loop: evaluate on query set\n",
        "            def forward_with_weights(x, weights):\n",
        "                x = nn.functional.conv2d(x, weights[0], weights[1], stride=2, padding=2)\n",
        "                x = nn.functional.relu(x)\n",
        "                x = nn.functional.conv2d(x, weights[2], weights[3], stride=2, padding=1)\n",
        "                x = nn.functional.relu(x)\n",
        "                x = nn.functional.conv2d(x, weights[4], weights[5], stride=2, padding=1)\n",
        "                x = nn.functional.relu(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = nn.functional.linear(x, weights[6], weights[7])\n",
        "                x = nn.functional.relu(x)\n",
        "                x = nn.functional.linear(x, weights[8], weights[9])\n",
        "                return x\n",
        "            logits_q = forward_with_weights(X_q, fast_weights)\n",
        "            loss_q = loss_fn(logits_q, y_q)\n",
        "            meta_loss += loss_q\n",
        "        meta_optimizer.zero_grad()\n",
        "        meta_loss.backward()\n",
        "        meta_optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}: Meta Loss = {meta_loss.item()/meta_batch_size:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "A5-an-WPne8h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Data Preparation and Meta-Task Definition\n",
        "\n",
        "# Load data (example for train file)\n",
        "with h5py.File(FILE_PATHS[0], \"r\") as f:\n",
        "    X = np.array(f[\"X_jets\"][:10000])  # Use a subset for speed\n",
        "    y = np.array(f[\"y\"][:10000])\n",
        "    pt = np.array(f[\"pt\"][:10000])\n",
        "\n",
        "# Define pt bins for meta-tasks (e.g., 5 bins)\n",
        "pt_bins = np.percentile(pt, np.linspace(0, 100, 6))\n",
        "print(\"pT bins:\", pt_bins)\n",
        "\n",
        "# Prepare dataset\n",
        "jet_dataset = JetImageDataset(X, y, pt=pt, pt_bins=pt_bins)\n",
        "\n",
        "# For baseline: use all data, random split\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "split = int(0.8 * len(indices))\n",
        "train_idx, val_idx = indices[:split], indices[split:]\n",
        "train_loader = DataLoader(Subset(jet_dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(Subset(jet_dataset, val_idx), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igD1nB40ng6C",
        "outputId": "8caf7ff2-f1c8-4f17-c1b7-b70ddd3c03a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pT bins: [ 70.55924225  95.12071228 106.02068024 117.48006592 135.62369995\n",
            " 302.24349976]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train and Evaluate Baseline\n",
        "\n",
        "baseline_model = JetCNN()\n",
        "baseline_model = train_baseline(baseline_model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPSsrcFjnisV",
        "outputId": "f2061d43-2c6d-4c52-f77b-cb837077f4dd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Val Acc = 0.6350\n",
            "Epoch 2: Val Acc = 0.6760\n",
            "Epoch 3: Val Acc = 0.6770\n",
            "Epoch 4: Val Acc = 0.6915\n",
            "Epoch 5: Val Acc = 0.6875\n",
            "Epoch 6: Val Acc = 0.6910\n",
            "Epoch 7: Val Acc = 0.6805\n",
            "Epoch 8: Val Acc = 0.6900\n",
            "Epoch 9: Val Acc = 0.6880\n",
            "Epoch 10: Val Acc = 0.6850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train and Evaluate MAML\n",
        "\n",
        "maml_model = JetCNN()\n",
        "maml_model = maml_train(maml_model, jet_dataset, pt_bins)"
      ],
      "metadata": {
        "id": "5evpzXnxnkwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df5fd29-9403-45e9-ede0-3ac434532239"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Meta Loss = 0.6928\n",
            "Epoch 2: Meta Loss = 0.7013\n",
            "Epoch 3: Meta Loss = 0.7049\n",
            "Epoch 4: Meta Loss = 0.8919\n",
            "Epoch 5: Meta Loss = 0.8584\n",
            "Epoch 6: Meta Loss = 1.0428\n",
            "Epoch 7: Meta Loss = 1.1687\n",
            "Epoch 8: Meta Loss = 1.1612\n",
            "Epoch 9: Meta Loss = 0.9326\n",
            "Epoch 10: Meta Loss = 0.7353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Few-Shot Evaluation on Held-Out Jet Conditions\n",
        "\n",
        "def few_shot_eval(model, dataset, pt_bins, n_tasks=5):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    for bin_idx in range(len(pt_bins)-1):\n",
        "        (X_s, y_s), (X_q, y_q) = sample_task(dataset, bin_idx, K_SHOT, K_QUERY)\n",
        "        X_s, y_s, X_q, y_q = X_s.to(DEVICE), y_s.to(DEVICE), X_q.to(DEVICE), y_q.to(DEVICE)\n",
        "        # Fast adaptation\n",
        "        fast_weights = [p.clone().detach().requires_grad_(True) for p in model.parameters()]\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for _ in range(INNER_STEPS):\n",
        "            logits = model(X_s)\n",
        "            loss = loss_fn(logits, y_s)\n",
        "            grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "            fast_weights = [w - INNER_LR * g for w, g in zip(fast_weights, grads)]\n",
        "        # Evaluate on query set\n",
        "        def forward_with_weights(x, weights):\n",
        "            x = nn.functional.conv2d(x, weights[0], weights[1], stride=2, padding=2)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = nn.functional.conv2d(x, weights[2], weights[3], stride=2, padding=1)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = nn.functional.conv2d(x, weights[4], weights[5], stride=2, padding=1)\n",
        "            x = nn.functional.relu(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = nn.functional.linear(x, weights[6], weights[7])\n",
        "            x = nn.functional.relu(x)\n",
        "            x = nn.functional.linear(x, weights[8], weights[9])\n",
        "            return x\n",
        "        logits_q = forward_with_weights(X_q, fast_weights)\n",
        "        preds = logits_q.argmax(dim=1)\n",
        "        acc = (preds == y_q).float().mean().item()\n",
        "        accs.append(acc)\n",
        "        print(f\"Bin {bin_idx}: Few-shot accuracy = {acc:.4f}\")\n",
        "    print(f\"Mean few-shot accuracy: {np.mean(accs):.4f}\")\n",
        "    return accs\n",
        "\n",
        "# Evaluate MAML\n",
        "few_shot_eval(maml_model, jet_dataset, pt_bins)\n",
        "# Evaluate Baseline (no adaptation, just forward pass)\n",
        "few_shot_eval(baseline_model, jet_dataset, pt_bins)"
      ],
      "metadata": {
        "id": "cP6DIU_-ljC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32b21fa-8293-4741-f97d-7c9b78f6c277"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bin 0: Few-shot accuracy = 0.6250\n",
            "Bin 1: Few-shot accuracy = 0.5000\n",
            "Bin 2: Few-shot accuracy = 0.3750\n",
            "Bin 3: Few-shot accuracy = 0.5000\n",
            "Bin 4: Few-shot accuracy = 0.3438\n",
            "Mean few-shot accuracy: 0.4688\n",
            "Bin 0: Few-shot accuracy = 0.5938\n",
            "Bin 1: Few-shot accuracy = 0.6562\n",
            "Bin 2: Few-shot accuracy = 0.7812\n",
            "Bin 3: Few-shot accuracy = 0.6250\n",
            "Bin 4: Few-shot accuracy = 0.7188\n",
            "Mean few-shot accuracy: 0.6750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.59375, 0.65625, 0.78125, 0.625, 0.71875]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}